{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights between the input and hidden layer. 3 input, 4 hidden neuron\n",
    "weights0 = 2*np.random.random((3,4)) - 1\n",
    "\n",
    "#weights between the hidden layer which consist of 4 neuron, and the ouptu which is one neuron (one scalar value). \n",
    "weights1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "network = [weights0,weights1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "def train(iterations, X_train, y_train,network):\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        l0 = X_train\n",
    "        l1 = sigmoid(np.dot(l0, network[0]))\n",
    "        output = sigmoid(np.dot(l1, network[1]))\n",
    "        #loss:        \n",
    "        output_error = y_train -  output #vector\n",
    "        #could be MSE: math.sqrt(sum((x - mean)**2 for x in numbers) / len(numbers) +1)  scalar #(this only needed if the output of one prediction is a vector!)\n",
    "        #but here are 4 scalars independently\n",
    "        #backpropagation\n",
    "        #calculate deltas       \n",
    "        output_delta = output_error*sigmoid_derivative(output)            \n",
    "        l1_error = output_delta.dot(network[1].T)\n",
    "        l1_delta = l1_error*sigmoid_derivative(l1)\n",
    "\n",
    "   #errors above are the loss functions  (Azért nem kell mean squered eror, mert az output az egy skalár?)\n",
    "   #delta-s are the gradient slopes, so when they close to zero, means the loss function is close to its minimum \n",
    "   #(because the function flats aout where the derivative is zero)\n",
    "   #delta literaly means gradient\n",
    "        \n",
    "        #update weights\n",
    "\n",
    "        network[1] += l1.T.dot(output_delta)\n",
    "        network[0] += l0.T.dot(l1_delta)\n",
    "    return network\n",
    "\n",
    "#bacthed training (i.e. if batch size is 1, there would be an other fro loop)\n",
    "\n",
    "Training\n",
    "def train(iterations, X_train, y_train,network):\n",
    "    for i in range(iterations):\n",
    "        output = predict(X_train, network)\n",
    "        error = y_train - output\n",
    "        #prevlayer = output   #(output is not a layer!)\n",
    "        \n",
    "        i = len(network) #backpropagation\n",
    "        while i <= 0:   \n",
    "            layer_delta = error*sigmoid_derivative(prevlayer)  \n",
    "            error = layer_delta.dot(network[i].T)\n",
    "            prevlayer = \n",
    "            i -= 1\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_train, network):         #no test data yet\n",
    "    layer = X_train\n",
    "    for weights in network:\n",
    "        layer = sigmoid(np.dot(layer, weights))\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],\n",
    "             [0,1,1],\n",
    "             [1,0,1],\n",
    "             [1,1,1]])\n",
    "\n",
    "y = np.array([[0], \n",
    "              [1] , \n",
    "              [1], \n",
    "              [0]])\n",
    "# which is the same as [0,1,1,0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_network = train(10000, X,y, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00851332]\n",
      " [0.98937834]\n",
      " [0.99127671]\n",
      " [0.01001158]]\n"
     ]
    }
   ],
   "source": [
    "print(predict(X,trained_network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
